Cache is a high speed storage/faster memory that stores a small proportion of frequent/critical data so that future requests can be served faster.

Caching is the process of storing data in a temporary storage called cache to improve speed of data access. 
A general strategy is to store the most frequent data in cache to avoid costly disk I/O operation, or recently used
data based on the context.

Client makes the request, first the cache is looked for the data, if present serve it, if there's a miss, then search in the
database -> serve request and update cache according to the strategy. 

Cache hit and miss metrics can be used for determining chache performance.

Cache eviction policy:
a. LRU: Least Recently Used
b. LFU: Least Frequently Used
c. FIFO: First in First Out
d. MRU: Most Recently Used
e. RR: Random Replacement 
or maybe other policy depending on the use case.

Cache Invalidation and Caching Strategies:
With constant update in database, it is also important to update cache to serve fresh data. There's majoorly 5 types of strategies
to solve cache invalidation:-
a. Cache-aside Strategy 
b. Read-through Strategy
c. Write-through Strategy
d. Write-around Strategy
e. Write-back Startegy

_____________________________________________

Cache Invalidation Strategies in nutshell -

a. Cache-aside Strategy:
Here Cache and DB are independent, and its the responsibility of application code to manage operations on cache and DB.
This is useful for read-heavy system. 
When user requests soe data, the system first checks the cache and serves the data if present, if data is not present, system will retrieve
the data from the database. After retrieving the system will store the fetched data in cache for future use.

b. Read-through Strategy:
In this Strategy, in the cache-miss scenario, unlike the cache-aside strategy the cache itself is responsible for updating the cache,
and returning the data to the application. This simplifies the application code by relieving it through from the complexity of cache management.

In both the above strategies, the cache can be inconsistent with the database in case of cache-hit. So we can use TTL for cache data, to
achieve eventual consistency.

To ensure immediate consistency, we can use the write strategies:

c. Write-through Strategy:
In this, the writes are first made to the cache and then to the DB, and write is successful if both places are updated.
This comes with a tradeoff of increased write latency, and hence not a good choice for write extensive aplications.
However, this is best for applications that frequently re-read data. Here the cache is always updated with latest data,
and write latency can be compensated. 

d. Write-around Strategy:
In this, the writes are directly written to the DB, and cache is updated only when read is called. This is highly useful for write-heavy
applications and may not be suited where recent data is re-read frequently. 

e. Write-back Strategy:
Here, all the writes are temporarily written to the cache and data is asynchronously updated to the Database. This strategy comes with the
risk when the cache layer fails. To overcome this multiple layers of cache can be used and data can be replicated.
_____________________________________________

Caching in System Design:
a. Browser Caching or client side caching - resources/pages are cached on user's browser. 
b. Web Server Caching - storing data on the server side
    - Reverse proxy cache : caches HTTP response, if this has the data, it serves the client without forwarding the request to the web server
    - Key value Database (memcached, redis) : unlike reverse proxy, it caches user-specific data.
c. CDN (Content Delivery Network) -
improves delivery speed by caching static content by residing in strategic location across the globe, thus minimizing distance between client
and server.
d. Distributed Caching
using multiple caching servers that are spread across a network. Unlike traditional caches which are limited to the memory of a single machine
distributed cache can scale beyond the memory limits of single machine by linking together multiple machines (distributed clusters)
Each caching server maintains a portion of cached data, and request are directed to appropriate cache server based on hasing algo or
some distribution strategy. 




Reference: https://www.enjoyalgorithms.com/blog/caching-system-design-concept/
